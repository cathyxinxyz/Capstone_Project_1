{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the collection of codes that read food atlas datasets and CDC health indicator datasets from Github repository, integrate datasets and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#merge food atlas datasets into one\n",
    "import pandas as pd\n",
    "\n",
    "dfs=list()\n",
    "url_folder='https://raw.githubusercontent.com/cathyxinxyz/Capstone_Project_1/master/Datasets/Food_atlas/'\n",
    "filenames=['ACCESS','ASSISTANCE','HEALTH','INSECURITY','LOCAL','PRICES_TAXES','RESTAURANTS','SOCIOECONOMIC','STORES']\n",
    "for i,filename in enumerate(filenames):\n",
    "    url=url_folder+filename+\".csv\"   \n",
    "    d=pd.read_csv(url,index_col='FIPS',encoding=\"ISO-8859-1\")\n",
    "    #append datasets to the list and drop the redundent columns:'State' and 'County'\n",
    "    if i!=0:\n",
    "        dfs.append(d.drop(['State', 'County'], axis=1))\n",
    "    else:\n",
    "        dfs.append(d)\n",
    "\n",
    "#merge datasets\n",
    "df_merge=pd.concat(dfs, join='outer', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (df_merge.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check columns for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merge.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_null_values_percol=df_merge.isnull().sum(axis=0)\n",
    "#columns with over 100 missing values\n",
    "cols_with_over_50_null_values=number_null_values_percol[number_null_values_percol>100]\n",
    "print (cols_with_over_50_null_values.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop these columns first\n",
    "df_merge=df_merge.drop(list(cols_with_over_50_null_values.index), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check number of remaining columns\n",
    "df_merge.shape\n",
    "print (df_merge.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categorizes columns into three groups: category data ('State' and 'County'), count data, percent data, # per 1000 pop, and percent change\n",
    "\n",
    "columns to keep: category data ('State' and 'County'), percent data, # per 1000 pop, and percent change; remove count data because it is not adjusted by population size\n",
    "\n",
    "Each column name is highly abstract and unreadable, need to extract info from the variable information provided by Food_atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "url='https://raw.githubusercontent.com/cathyxinxyz/Capstone_Project_1/master/Datasets/Food_atlas/variable_info.csv'\n",
    "var_info_df=pd.read_csv(url,encoding=\"ISO-8859-1\")\n",
    "var_info_dict=defaultdict(list)\n",
    "print (var_info_df.head(5))\n",
    "for idx in var_info_df.index:\n",
    "    k=var_info_df['Units'][idx]\n",
    "    var_info_dict[k].append(var_info_df['Variable Code'][idx])\n",
    "\n",
    "print (var_info_dict.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further filter varaibles based on following principles:\n",
    "i. keep variables that are adjusted by population size: '% change', 'Percent', '# per 1,000 pop','Percentage points';\n",
    "ii. keep variables that are mostly valuable for analysis\n",
    "iii. keep variables where values are valid: e.g. no negative values for variables with units as 'Percent' or '# per 1,000 pop'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#units to keep: '% change', 'Percent', '# per 1,000 pop','Percentage points'\n",
    "for k in var_info_dict.keys():\n",
    "    if k not in ['% change', 'Percent', '# per 1,000 pop','Percentage points'] and var_info_dict[k] not in ['State','County']:\n",
    "        df_merge = df_merge[df_merge.columns.difference(var_info_dict[k])]\n",
    "        \n",
    "#print (df_merge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#view variables\n",
    "for idx in var_info_df.index:\n",
    "    k=var_info_df['Units'][idx]\n",
    "    k1=var_info_df['Category Code'][idx]\n",
    "    k2=var_info_df['Sub_subcategory Name'][idx]\n",
    "    var=var_info_df['Variable Code'][idx]\n",
    "    \n",
    "    if var in df_merge.columns:\n",
    "        print (k1,k2,k,var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_merge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#subset of variables chosen for analysis\n",
    "#dropped variables: \n",
    "#                   ASSISTANCE for students: PCT_NSLP09, PCT_NSLP15,PCH_NSLP_09_15,PCT_FREE_LUNCH09,PCT_REDUCED_LUNCH09,PCT_SBP09,\n",
    "#                                            PCT_SBP15,PCH_SBP_09_15,PCT_SFSP09,PCT_SFSP15,PCH_SFSP_09_15\n",
    "#                   ASSISTANCE SNAP participants/eligible pop Percent:SNAP_PART_RATE08 and SNAP_PART_RATE13\n",
    "#                   PRICES_TAXES: SODATAX_STORES14, SODATAX_VENDM14, SODATAX_VENDM14, CHIPSTAX_STORES14, FOOD_TAX14,CHIPSTAX_VENDM14\n",
    "#                   HEALTH Adult diabetes rate: PCT_DIABETES_ADULTS08, PCT_DIABETES_ADULTS13, PCT_OBESE_ADULTS08, PCT_OBESE_ADULTS13\n",
    "#                   \n",
    "\n",
    "df_merge=df_merge.drop(['PCT_NSLP09', 'PCT_NSLP15', 'PCH_NSLP_09_15','PCT_FREE_LUNCH09',\n",
    "                        'PCT_REDUCED_LUNCH09','PCT_SBP09','PCT_SBP15','PCH_SBP_09_15',\n",
    "                        'PCT_SFSP09','PCT_SFSP15','PCH_SFSP_09_15',\n",
    "                        'SNAP_PART_RATE08','SNAP_PART_RATE13',\n",
    "                        'SODATAX_STORES14', 'SODATAX_VENDM14', 'SODATAX_VENDM14', \n",
    "                        'CHIPSTAX_STORES14', 'FOOD_TAX14','CHIPSTAX_VENDM14',\n",
    "                         'PCT_DIABETES_ADULTS08', 'PCT_DIABETES_ADULTS13', 'PCT_OBESE_ADULTS08', 'PCT_OBESE_ADULTS13'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3143, 96)\n",
      "ACCESS nan % change PCH_LACCESS_POP_10_15\n",
      "ACCESS nan Percent PCT_LACCESS_POP10\n",
      "ACCESS nan Percent PCT_LACCESS_POP15\n",
      "ACCESS Low income % change PCH_LACCESS_LOWI_10_15\n",
      "ACCESS Low income Percent PCT_LACCESS_LOWI10\n",
      "ACCESS Low income Percent PCT_LACCESS_LOWI15\n",
      "ACCESS no car % change PCH_LACCESS_HHNV_10_15\n",
      "ACCESS no car Percent PCT_LACCESS_HHNV10\n",
      "ACCESS no car Percent PCT_LACCESS_HHNV15\n",
      "ACCESS SNAP Percent PCT_LACCESS_SNAP15\n",
      "ACCESS Children Percent PCT_LACCESS_CHILD10\n",
      "ACCESS Children Percent PCT_LACCESS_CHILD15\n",
      "ACCESS Seniors % change PCH_LACCESS_SENIORS_10_15\n",
      "ACCESS Seniors Percent PCT_LACCESS_SENIORS10\n",
      "ACCESS Seniors Percent PCT_LACCESS_SENIORS15\n",
      "ACCESS White Percent PCT_LACCESS_WHITE15\n",
      "ACCESS Black Percent PCT_LACCESS_BLACK15\n",
      "ACCESS Hispanic ethnicity Percent PCT_LACCESS_HISP15\n",
      "ACCESS Asian Percent PCT_LACCESS_NHASIAN15\n",
      "ACCESS American Indian or Alaska Native Percent PCT_LACCESS_NHNA15\n",
      "ACCESS Hawaiian or Pacific Islander Percent PCT_LACCESS_NHPI15\n",
      "ACCESS Multiracial Percent PCT_LACCESS_MULTIR15\n",
      "STORES Grocery % change PCH_GROC_09_14\n",
      "STORES Grocery # per 1,000 pop GROCPTH09\n",
      "STORES Grocery # per 1,000 pop GROCPTH14\n",
      "STORES Supercenters # per 1,000 pop SUPERCPTH09\n",
      "STORES Supercenters # per 1,000 pop SUPERCPTH14\n",
      "STORES Convenience % change PCH_CONVS_09_14\n",
      "STORES Convenience # per 1,000 pop CONVSPTH09\n",
      "STORES Convenience # per 1,000 pop CONVSPTH14\n",
      "STORES Convenience % change PCH_CONVSPTH_09_14\n",
      "STORES Specialized # per 1,000 pop SPECSPTH09\n",
      "STORES Specialized # per 1,000 pop SPECSPTH14\n",
      "STORES SNAP-authorized % change PCH_SNAPS_12_16\n",
      "STORES SNAP-authorized # per 1,000 pop SNAPSPTH12\n",
      "STORES SNAP-authorized # per 1,000 pop SNAPSPTH16\n",
      "STORES SNAP-authorized % change PCH_SNAPSPTH_12_16\n",
      "STORES WIC-authorized % change PCH_WICS_08_12\n",
      "STORES WIC-authorized # per 1,000 pop WICSPTH08\n",
      "STORES WIC-authorized # per 1,000 pop WICSPTH12\n",
      "STORES WIC-authorized % change PCH_WICSPTH_08_12\n",
      "RESTAURANTS Fast-food % change PCH_FFR_09_14\n",
      "RESTAURANTS Fast-food # per 1,000 pop FFRPTH09\n",
      "RESTAURANTS Fast-food # per 1,000 pop FFRPTH14\n",
      "RESTAURANTS Fast-food % change PCH_FFRPTH_09_14\n",
      "RESTAURANTS Full-service % change PCH_FSR_09_14\n",
      "RESTAURANTS Full-service # per 1,000 pop FSRPTH09\n",
      "RESTAURANTS Full-service # per 1,000 pop FSRPTH14\n",
      "ASSISTANCE SNAP participants Percent PCT_SNAP12\n",
      "ASSISTANCE SNAP participants  Percent PCT_SNAP16\n",
      "ASSISTANCE SNAP participants Percentage points PCH_SNAP_12_16\n",
      "ASSISTANCE SNAP benefits per capita % change PCH_PC_SNAPBEN_10_15\n",
      "ASSISTANCE National School Lunch Program participants  Percent PCT_NSLP09\n",
      "ASSISTANCE National School Lunch Program participants  Percent PCT_NSLP15\n",
      "ASSISTANCE National School Lunch Program participants  Percentage points PCH_NSLP_09_15\n",
      "ASSISTANCE Students eligible for free lunch Percent PCT_FREE_LUNCH09\n",
      "ASSISTANCE Students eligible for reduced-price lunch Percent PCT_REDUCED_LUNCH09\n",
      "ASSISTANCE School Breakfast Program participants Percent PCT_SBP09\n",
      "ASSISTANCE School Breakfast Program participants Percent PCT_SBP15\n",
      "ASSISTANCE School Breakfast Program participants Percentage points PCH_SBP_09_15\n",
      "ASSISTANCE Summer Food Program participants Percent PCT_SFSP09\n",
      "ASSISTANCE Summer Food Program participants Percent PCT_SFSP15\n",
      "ASSISTANCE Summer Food Program participants Percentage points PCH_SFSP_09_15\n",
      "ASSISTANCE WIC participants Percent PCT_WIC09\n",
      "ASSISTANCE WIC participants Percent PCT_WIC15\n",
      "ASSISTANCE WIC participants Percentage points PCH_WIC_09_15\n",
      "ASSISTANCE nan Percent PCT_CACFP09\n",
      "ASSISTANCE nan Percent PCT_CACFP15\n",
      "ASSISTANCE nan Percentage points PCH_CACFP_09_15\n",
      "INSECURITY Household food insecurity Percent FOODINSEC_10_12\n",
      "INSECURITY Household food insecurity Percent FOODINSEC_13_15\n",
      "INSECURITY Household food insecurity Percentage points CH_FOODINSEC_12_15\n",
      "INSECURITY Household very low food security Percent VLFOODSEC_10_12\n",
      "INSECURITY Household very low food security Percent VLFOODSEC_13_15\n",
      "INSECURITY Household very low food security Percentage points CH_VLFOODSEC_12_15\n",
      "INSECURITY Household child food insecurity  Percent FOODINSEC_CHILD_01_07\n",
      "INSECURITY Household child food insecurity  Percent FOODINSEC_CHILD_03_11\n",
      "LOCAL Farms Percent PCT_LOCLFARM07\n",
      "LOCAL Farms Percent PCT_LOCLFARM12\n",
      "LOCAL overall # per 1,000 pop FMRKTPTH09\n",
      "LOCAL overall # per 1,000 pop FMRKTPTH16\n",
      "HEALTH Recreation & fitness facilities # per 1,000 pop RECFACPTH09\n",
      "HEALTH Recreation & fitness facilities # per 1,000 pop RECFACPTH14\n",
      "SOCIOECONOMIC White Percent PCT_NHWHITE10\n",
      "SOCIOECONOMIC Black Percent PCT_NHBLACK10\n",
      "SOCIOECONOMIC Hispanic Percent PCT_HISP10\n",
      "SOCIOECONOMIC Asian Percent PCT_NHASIAN10\n",
      "SOCIOECONOMIC American Indian or Alaska Native Percent PCT_NHNA10\n",
      "SOCIOECONOMIC Hawaiian or Pacific Islander Percent PCT_NHPI10\n",
      "SOCIOECONOMIC >=65 Percent PCT_65OLDER10\n",
      "SOCIOECONOMIC <18 Percent PCT_18YOUNGER10\n",
      "SOCIOECONOMIC Poverty rate Percent POVRATE15\n",
      "SOCIOECONOMIC Child poverty rate Percent CHILDPOVRATE15\n"
     ]
    }
   ],
   "source": [
    "print(df_merge.shape)\n",
    "#view variables\n",
    "for idx in var_info_df.index:\n",
    "    k=var_info_df['Units'][idx]\n",
    "    k1=var_info_df['Category Code'][idx]\n",
    "    k2=var_info_df['Sub_subcategory Name'][idx]\n",
    "    var=var_info_df['Variable Code'][idx]\n",
    "    \n",
    "    if var in df_merge.columns:\n",
    "        print (k1,k2,k,var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devide dataframe into two: one with variables measured at one year and one with variables as percent changeã€\n",
    "var_timepoint=list()\n",
    "var_percentchange=list()\n",
    "for idx in var_info_df.index:\n",
    "    k=var_info_df['Units'][idx]\n",
    "    k1=var_info_df['Category Code'][idx]\n",
    "    k2=var_info_df['Sub_subcategory Name'][idx]\n",
    "    var=var_info_df['Variable Code'][idx]\n",
    "    \n",
    "    if var in df_merge.columns: \n",
    "        if k in ['Percent', '# per 1,000 pop']:\n",
    "            var_timepoint.append(var)\n",
    "        elif k in ['% change','Percentage points']:\n",
    "            var_percentchange.append(var)\n",
    "var_timepoint.extend(['State','County'])\n",
    "var_percentchange.extend(['State','County'])\n",
    "df_tp=df_merge[var_timepoint]\n",
    "df_pr=df_merge[var_percentchange]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_tp.shape)\n",
    "print (df_pr.shape)\n",
    "print (df_tp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check weather each column has valid values:\n",
    "####### columns with units 'Percent' should have values between 0 and 100, any value that fall out of this range should be changed to NaN values\n",
    "###### \n",
    "######\n",
    "######\n",
    "\n",
    "#Replace invalid values with np.nan\n",
    "import numpy as np\n",
    "\n",
    "for idx in var_info_df.index:\n",
    "    k=var_info_df['Units'][idx]\n",
    "    k1=var_info_df['Category Code'][idx]\n",
    "    k2=var_info_df['Sub_subcategory Name'][idx]\n",
    "    var=var_info_df['Variable Code'][idx]\n",
    "    \n",
    "    if k =='Percent' and var in df_tp.columns: \n",
    "        ser=df_tp[var]<0 \n",
    "        ser=ser+df_tp[var]>100\n",
    "        if ser.sum()>0:\n",
    "            print ((k1,k2,var,ser.sum()))\n",
    "            df_tp[var][df_tp[var]<0 or df_tp[var]>100]=np.nan\n",
    "    elif k=='# per 1,000 pop' and var in df_tp.columns:\n",
    "        ser=df_tp[var]<0\n",
    "        ser=ser+df_tp[var]>1000\n",
    "        if ser.sum()>0:\n",
    "            print ((k1,k2,var,ser.sum()))\n",
    "            df_tp[var][df_tp[var]<0 or df_tp[var]>1000]=np.nan\n",
    "    elif k=='Percentage points' and var in df_pr.columns:\n",
    "        ser=df_pr[var]>100\n",
    "        ser=df_pr[var]<-100\n",
    "        if ser.sum()>0:\n",
    "            print ((k1,k2,var,ser.sum())) \n",
    "            df_pr[var][df_pr[var]<-100 or df_pr[var]>100]=np.nan\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#break df_tp into two sets: variables measured at the earlier time point: df_tp_1; and variables measured at the earlier time point: df_tp_2\n",
    "\n",
    "#group the same measure into tuples, the same measure share the same name except the last two digits which indicate the year of the measure\n",
    "var_grouped_by_measures=defaultdict(list)\n",
    "early_measure_list=['State','County']\n",
    "late_measure_list=['State','County']\n",
    "for idx in var_info_df.index:\n",
    "    k=var_info_df['Units'][idx]\n",
    "    k1=var_info_df['Category Code'][idx]\n",
    "    k2=var_info_df['Sub_subcategory Name'][idx]\n",
    "    var=var_info_df['Variable Code'][idx]\n",
    "    \n",
    "    if var in df_tp.columns and var not in ['State','County']:\n",
    "        var_grouped_by_measures[(k1,k2)].append((var, float(var[-2:])))\n",
    "    \n",
    "for v in var_grouped_by_measures.values():\n",
    "    v.sort(key=lambda tup: tup[-1])\n",
    "    early_measure_list.append(v[0][0])  \n",
    "    late_measure_list.append(v[-1][0])\n",
    "\n",
    "df_tp_1=df_tp[early_measure_list]\n",
    "df_tp_2=df_tp[late_measure_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (df_tp_1.shape)\n",
    "print (df_tp_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tp_1.to_csv('H:/Github/Capstone_Project_1/Datasets/Food_atlas/df_tp_1.csv')\n",
    "df_tp_2.to_csv('H:/Github/Capstone_Project_1/Datasets/Food_atlas/df_tp_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pr.to_csv('H:/Github/Capstone_Project_1/Datasets/Food_atlas/df_pr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate CDC Datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
